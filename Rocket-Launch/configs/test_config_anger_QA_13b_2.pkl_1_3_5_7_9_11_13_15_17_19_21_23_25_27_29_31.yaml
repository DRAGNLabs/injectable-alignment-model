tokenizer_type: sp
tokenizer_path: /grphome/grp_inject/compute/tokenizer.model
pad_id: -1
vocab_size: -1
IRM_layers: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]
loss_function_index: 0
default_root_dir: /home/myl15/inject/injectable-alignment-model/Rocket-Launch/runs/test_config_1_3_5_7_9_11_13_15_17_19_21_23_25_27_29_31/
checkpoint_path: grphome/grp_inject/compute/irm_runs_that_are_trash/config_Llama-2-7b-chat-hf_anger_QA_7b.pkl_0_1_2_3_4_5_6_7/checkpoints/model-epoch=0-val_loss=3.05.ckpt
raw_train_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
raw_test_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
raw_val_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
tokenized_dataset_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
train_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
test_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
eval_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
accelerator: gpu
num_nodes: 2
num_workers: 0
devices: 2
use_slurm: true
log_every_n_steps: 200
check_val_every_n_epoch: 1
val_check_interval: 0.2
batch_size: 8
gradient_accumulation_steps: 1
num_epochs: 15
lr: 0.0001
gamma: 0.85
seed: 42
early_stopping: 6
save_top_k: 3
save_predictions_during_training: true
inference_path: /home/myl15/inject/injectable-alignment-model/Rocket-Launch/dataset/raw/inference_text.txt
max_gen_len: 20
from_pretrained: false
model_name: ~
experiment_name: "test"
do_logging: True
# attention_bias: false
# attention_dropout: 0.0
# bos_token_id: 1
# eos_token_id: 2
# hidden_act: silu
# hidden_size: 4096
# initializer_range: 0.02
# intermediate_size: 11008
# max_position_embeddings: 128
# model_type: llama
# num_attention_heads: 32
num_hidden_layers: 32
# num_key_value_heads: 32
# pretraining_tp: 1
# rms_norm_eps: 0.0001
# rope_scaling: ~
# rope_theta: 10000.0
# tie_word_embeddings: false
# torch_dtype: float16
# transformers_version: 4.38.2
# use_cache: true




model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 128
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: 0.0001
  rope_scaling: ~
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: float16
  transformers_version: 4.38.2
  use_cache: true
  vocab_size: 32000
