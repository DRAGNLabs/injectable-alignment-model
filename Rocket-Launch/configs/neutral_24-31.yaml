tokenizer_type: sp
tokenizer_path: /grphome/grp_inject/compute/tokenizer.model
pad_id: -1
vocab_size: -1
IRM_layers: [24,25,26,27,28,29,30,31]
loss_function_index: 0
default_root_dir: /home/myl15/inject/injectable-alignment-model/Rocket-Launch/runs/test_config_boy_Llama-2-7b-hf_anger_QA_13b_2.pkl_0_1_2_3_4/
checkpoint_path: /home/myl15/fsl_groups/grp_inject/compute/irm_runs_that_are_trash/config_Llama-2-7b-chat-hf_neutral_QA_7b.pkl_24_25_26_27_28_29_30_31/checkpoints/model-epoch=5-val_loss=2.69.ckpt
raw_train_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
raw_test_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
raw_val_path: /grphome/grp_inject/injectable-alignment-model/dataset/raw/anger_QA_13b_2.csv
tokenized_dataset_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
train_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
test_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
eval_path: /grphome/grp_inject/injectable-alignment-model/dataset/tokenized/anger_QA_13b_2.pkl
accelerator: gpu
num_nodes: 2
num_workers: 0
devices: 2
use_slurm: true
log_every_n_steps: 200
check_val_every_n_epoch: 1
val_check_interval: 0.2
batch_size: 8
gradient_accumulation_steps: 1
num_epochs: 15
lr: 0.0001
gamma: 0.85
seed: 42
early_stopping: 6
save_top_k: 3
save_predictions_during_training: true
inference_path: /home/myl15/inject/injectable-alignment-model/Rocket-Launch/dataset/raw/inference_text.txt
max_gen_len: 20
do_logging: true
experiment_name: neutral_test_24-31
from_pretrained: false
model_name: ~
num_hidden_layers: 32

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 4096
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: 0.0001
  rope_scaling: ~
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: float16
  transformers_version: 4.38.2
  use_cache: true
  vocab_size: 32000
