tokenizer_type: hf
tokenizer_path: meta-llama/Llama-2-7b-chat-hf
pad_id: -1
vocab_size: -1
IRM_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
loss_function_index: 0
default_root_dir: /home/dfbaker5/cs301r/irm_sanbox/injectable-alignment-model/runs/Llama-2-7b-chat-hf_anger_60k_0-31/
checkpoint_path: /grphome/grp_inject/compute/hf_weights/hf_llama_7b.ckpt
dataset_dir: /home/dfbaker5/cs301r/irm_sanbox/injectable-alignment-model/dataset/anger_QA_7b_60k/
dataset_name: anger_60k
accelerator: gpu
num_nodes: 1
num_workers: 1
devices: 2
use_slurm: true
log_every_n_steps: 200
check_val_every_n_epoch: 1
val_check_interval: 0.25
batch_size: 8
gradient_accumulation_steps: 1
num_epochs: 15
lr: 0.0001
gamma: 0.85
seed: 42
early_stopping: 6
save_top_k: 3
save_predictions_during_training: true
regularize_loss: false
max_gen_len: 20
do_logging: false
experiment_name: Llama-2-7b-chat-hf_anger_60k_0-31
from_pretrained: true
model_name: meta-llama/Llama-2-7b-chat-hf

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 128
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: .00001
  rope_scaling: ~
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: float16
  transformers_version: 4.38.2
  use_cache: true
  vocab_size: 32000
