from collections import defaultdict
>>> def extract_llama_layer_weights(model):
...     weights = defaultdict(list)
...
...     for i, layer in enumerate(model.model.layers):
...         # Extract self-attention weights
...         weights['q_proj'].append(layer.self_attn.q_proj.weight.data)
...         weights['k_proj'].append(layer.self_attn.k_proj.weight.data)
...         weights['v_proj'].append(layer.self_attn.v_proj.weight.data)
...         weights['o_proj'].append(layer.self_attn.o_proj.weight.data)
...
...         # Extract MLP weights
...         weights['gate_proj'].append(layer.mlp.gate_proj.weight.data)
...         weights['up_proj'].append(layer.mlp.up_proj.weight.data)
...         weights['down_proj'].append(layer.mlp.down_proj.weight.data)
...
...         # Extract normalization weights
...         weights['input_layernorm'].append(layer.input_layernorm.weight.data)
...         weights['post_attention_layernorm'].append(layer.post_attention_layernorm.weight.data)
...
...     return weights
...
>>> hf_weights = extract_llama_layer_weights(hf_model)

def extract_transformer_block_weights(model):
...     weights = defaultdict(list)
...
...     for i, block in enumerate(model.layers):
...         # Extract attention weights
...         weights['wq'].append(block.attention.wq.weight.data)
...         weights['wk'].append(block.attention.wk.weight.data)
...         weights['wv'].append(block.attention.wv.weight.data)
...         weights['wo'].append(block.attention.wo.weight.data)
...
...         # Extract feed forward weights
...         weights['w1'].append(block.feed_forward.w1.weight.data)
...         weights['w2'].append(block.feed_forward.w2.weight.data)
...         weights['w3'].append(block.feed_forward.w3.weight.data)
...
...         # Extract normalization weights
...         weights['attention_norm'].append(block.attention_norm.weight.data)
...         weights['ffn_norm'].append(block.ffn_norm.weight.data)
...
...     return weights
...
>>> my_weights = extract_transformer_block_weights(my_model)


for i in range(32):
...     my_wq = my_weights['wq'][i]
...     hf_wq = hf_weights['q_proj'][i]
...     mse = ((my_wq - hf_wq) ** 2).mean()
...     mae = torch.mean(torch.abs(my_wq - hf_wq))
...     cos = F.cosine_similarity(my_wq, hf_wq, dim=0)
...     print(f"mse:{mse}, mae: {mae}, cos: {cos}")



mse:0.0006256103515625, mae: 0.01385498046875, cos: tensor([ 0.0013, -0.0011, -0.0156,  ...,  0.0183,  0.0520,  0.0054])
mse:0.000667572021484375, mae: 0.0189208984375, cos: tensor([0.0166, 0.0120, 0.0122,  ..., 0.0300, 0.0026, 0.0056])
mse:0.00060272216796875, mae: 0.0184326171875, cos: tensor([ 0.0178,  0.0075,  0.0101,  ...,  0.0520, -0.0073,  0.0383])
mse:0.0006103515625, mae: 0.0189208984375, cos: tensor([0.0325, 0.0001, 0.0330,  ..., 0.0452, 0.0264, 0.0208])
mse:0.0005950927734375, mae: 0.0185546875, cos: tensor([ 0.0308, -0.0089,  0.0181,  ..., -0.0177,  0.0206,  0.0273])
mse:0.00058746337890625, mae: 0.018310546875, cos: tensor([ 0.0776, -0.0102,  0.0096,  ...,  0.0192, -0.0130, -0.0106])
mse:0.0006103515625, mae: 0.0189208984375, cos: tensor([ 0.0161,  0.0388, -0.0068,  ...,  0.0410,  0.0162,  0.0125])
mse:0.0005645751953125, mae: 0.01806640625, cos: tensor([-0.0005,  0.0096,  0.0177,  ...,  0.0183,  0.0064,  0.0027])
mse:0.0005645751953125, mae: 0.0179443359375, cos: tensor([ 0.0068,  0.0232,  0.0018,  ..., -0.0078, -0.0214,  0.0303])
mse:0.0005645751953125, mae: 0.01806640625, cos: tensor([ 0.0255,  0.0222,  0.0305,  ...,  0.0126, -0.0001,  0.0356])
mse:0.00058746337890625, mae: 0.0181884765625, cos: tensor([0.0175, 0.0149, 0.0216,  ..., 0.0425, 0.0302, 0.0101])
mse:0.00052642822265625, mae: 0.0174560546875, cos: tensor([-0.0070,  0.0479, -0.0127,  ...,  0.0496,  0.0040,  0.0118])
mse:0.00054168701171875, mae: 0.0179443359375, cos: tensor([0.0186, 0.0170, 0.0260,  ..., 0.0267, 0.0063, 0.0214])
mse:0.000553131103515625, mae: 0.017822265625, cos: tensor([ 0.0322,  0.0188,  0.0081,  ...,  0.0194, -0.0229,  0.0226])
mse:0.000530242919921875, mae: 0.0174560546875, cos: tensor([0.0439, 0.0312, 0.0101,  ..., 0.0040, 0.0288, 0.0078])
mse:0.00064849853515625, mae: 0.01904296875, cos: tensor([ 0.0106,  0.0073,  0.0216,  ...,  0.0125,  0.0114, -0.0217])
mse:0.00060272216796875, mae: 0.0186767578125, cos: tensor([-0.0094,  0.0277,  0.0134,  ..., -0.0043,  0.0121,  0.0557])
mse:0.000614166259765625, mae: 0.018798828125, cos: tensor([ 0.0325,  0.0425,  0.0452,  ...,  0.0156,  0.0012, -0.0082])
mse:0.0005950927734375, mae: 0.018798828125, cos: tensor([ 0.0131,  0.0398, -0.0211,  ...,  0.0109, -0.0089,  0.0090])
mse:0.0005950927734375, mae: 0.0186767578125, cos: tensor([ 0.0019,  0.0284,  0.0239,  ...,  0.0280, -0.0171,  0.0115])
mse:0.000583648681640625, mae: 0.0185546875, cos: tensor([ 0.0356,  0.0132,  0.0079,  ...,  0.0023, -0.0108,  0.0139])
mse:0.000568389892578125, mae: 0.0184326171875, cos: tensor([ 0.0049,  0.0002, -0.0120,  ...,  0.0532,  0.0118,  0.0031])
mse:0.00054168701171875, mae: 0.01806640625, cos: tensor([ 0.0087, -0.0079,  0.0262,  ..., -0.0061,  0.0299,  0.0040])
mse:0.000545501708984375, mae: 0.01806640625, cos: tensor([ 0.0141, -0.0003, -0.0075,  ...,  0.0125,  0.0073,  0.0013])
mse:0.00052642822265625, mae: 0.0177001953125, cos: tensor([ 0.0023,  0.0261, -0.0033,  ..., -0.0032,  0.0386,  0.0037])
mse:0.00051116943359375, mae: 0.0174560546875, cos: tensor([0.0150, 0.0253, 0.0527,  ..., 0.0125, 0.0247, 0.0330])
mse:0.000514984130859375, mae: 0.0174560546875, cos: tensor([-0.0069,  0.0048, -0.0148,  ...,  0.0461,  0.0035, -0.0233])
mse:0.000499725341796875, mae: 0.0172119140625, cos: tensor([0.0452, 0.0157, 0.0008,  ..., 0.0527, 0.0110, 0.0069])
mse:0.0005035400390625, mae: 0.017333984375, cos: tensor([0.0044, 0.0031, 0.0114,  ..., 0.0334, 0.0153, 0.0079])
mse:0.00049591064453125, mae: 0.0172119140625, cos: tensor([ 0.0393,  0.0016,  0.0029,  ..., -0.0063,  0.0374,  0.0249])
mse:0.00045013427734375, mae: 0.01611328125, cos: tensor([ 0.0087, -0.0050,  0.0309,  ..., -0.0036,  0.0410,  0.0214])
mse:0.000545501708984375, mae: 0.017822265625, cos: tensor([-0.0143, -0.0203,  0.0131,  ...,  0.0071,  0.0327,  0.0112])
>>> my_wq = my_weights['wq'][1]
>>> my_wq
tensor([[-0.0312, -0.0181, -0.0189,  ..., -0.0281,  0.0028, -0.0088],
        [ 0.0117, -0.0277,  0.0303,  ...,  0.0098, -0.0137,  0.0298],
        [-0.0029,  0.0005, -0.0078,  ..., -0.0229,  0.0010,  0.0079],
        ...,
        [-0.0198, -0.0222, -0.0427,  ...,  0.0391,  0.0234,  0.0153],
        [-0.0175,  0.0092, -0.0042,  ...,  0.0292,  0.0050,  0.0405],
        [ 0.0223,  0.0276, -0.0021,  ..., -0.0130, -0.0068, -0.0150]])
>>> hf_wq = hf_weights['q_proj'][1]
>>> hf_wq
tensor([[-0.0312, -0.0181, -0.0189,  ..., -0.0281,  0.0028, -0.0088],
        [-0.0029,  0.0005, -0.0078,  ..., -0.0229,  0.0010,  0.0079],
        [-0.0032,  0.0089, -0.0236,  ..., -0.0123, -0.0021, -0.0083],
        ...,
        [ 0.0559,  0.0029,  0.0017,  ...,  0.0064,  0.0137, -0.0369],
        [-0.0198, -0.0222, -0.0427,  ...,  0.0391,  0.0234,  0.0153],
        [ 0.0223,  0.0276, -0.0021,  ..., -0.0130, -0.0068, -0.0150]])


>>> my_wq_unique = np.unique(torch.sum(my_wq,dim=-1).to(torch.float32).cpu())
>>> hf_wq_unique = np.unique(torch.sum(hf_wq,dim=-1).to(torch.float32).cpu())
>>> hf_wq_unique
array([-6.21875, -4.8125 , -4.59375, ...,  5.46875,  5.71875,  6.84375],
      dtype=float32)
>>> my_wq_unique == hf_wq_unique
array([ True,  True,  True, ...,  True,  True,  True])
>>> my_wq_unique != hf_wq_unique
array([False, False, False, ..., False, False, False])
>>> diff = my_wq_unique != hf_wq_unique
>>> np.any(diff)
False
