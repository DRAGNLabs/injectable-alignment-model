{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-14T11:11:06.830899Z","iopub.status.busy":"2024-09-14T11:11:06.830617Z","iopub.status.idle":"2024-09-14T11:11:22.602545Z","shell.execute_reply":"2024-09-14T11:11:22.601443Z","shell.execute_reply.started":"2024-09-14T11:11:06.830866Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tiktoken in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (0.7.0)\n","Requirement already satisfied: blobfile in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (3.0.0)\n","Requirement already satisfied: regex>=2022.1.18 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: pycryptodomex>=3.8 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from blobfile) (3.21.0)\n","Requirement already satisfied: urllib3<3,>=1.25.3 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from blobfile) (2.2.0)\n","Requirement already satisfied: lxml>=4.9 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from blobfile) (5.3.0)\n","Requirement already satisfied: filelock>=3.0 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from blobfile) (3.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/huang717/.conda/envs/irm/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"]}],"source":["!pip3 install tiktoken blobfile"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:22.605420Z","iopub.status.busy":"2024-09-14T11:11:22.604988Z","iopub.status.idle":"2024-09-14T11:11:25.648079Z","shell.execute_reply":"2024-09-14T11:11:25.646951Z","shell.execute_reply.started":"2024-09-14T11:11:22.605354Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/huang717/.conda/envs/irm/lib/python3.10/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n","  _C._set_default_tensor_type(t)\n"]}],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.set_default_tensor_type(torch.BFloat16Tensor) # Otherwise it may not fit into memory."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:25.650358Z","iopub.status.busy":"2024-09-14T11:11:25.649831Z","iopub.status.idle":"2024-09-14T11:11:25.656441Z","shell.execute_reply":"2024-09-14T11:11:25.655378Z","shell.execute_reply.started":"2024-09-14T11:11:25.650298Z"},"trusted":true},"outputs":[],"source":["DIM = 4096 # Llama3 Table 3.\n","FFN_DIM = 14336 # For POSITION-WISE FEED-FORWARD NETWORK (FFN) Llama Table 3\n","N_LAYERS = 32 # Llama3 Table 3.\n","N_HEADS = 32 # Llama3 Table 3.\n","N_KV_HEADS = 8 # With 8 key-value heads to improve inference speed and to reduce the size (llama3)\n","VOCAB_SIZE = 128256 # Llama3 vocab size. Llama3 paper says 128K. This is the exact number taken from tokenizer.\n","NORM_EPS = 1e-5 # Took from Llama3 code ModelArgs.\n","ROPE_THETA = 500000 # We increase the RoPE base frequency hyperparameter to 500,000 (llama3)\n","MAX_BATCH_SIZE = 16 # Just optional depending on your specs. If number of examples you provide is smaller, it takes it as batch size.\n","MAX_SEQ_LEN = 1280 # Just optional depending on your specs.\n","N_KV_HEAD_REP = N_HEADS // N_KV_HEADS # How many times you repeat KV to match your queries(N_HEADS).\n","HEAD_DIM = DIM // N_HEADS # Divide dimension by number of heads to get dimension per head."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:25.659602Z","iopub.status.busy":"2024-09-14T11:11:25.659211Z","iopub.status.idle":"2024-09-14T11:11:25.794218Z","shell.execute_reply":"2024-09-14T11:11:25.792424Z","shell.execute_reply.started":"2024-09-14T11:11:25.659558Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 8, 4096])\n","------------------------------\n","torch.Size([2, 8, 32, 128])\n","torch.Size([2, 8, 8, 128])\n"]}],"source":["# Apply pre-normalization using RMSNorm (llama2)\n","class RMSNorm(torch.nn.Module):\n","    def __init__(self, dim, norm_eps):\n","        super().__init__()\n","        self.norm_eps = norm_eps\n","        self.weight = nn.Parameter(torch.ones(dim))\n","\n","    def _norm(self, x):\n","        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n","    \n","    def forward(self, x):\n","        out = self._norm(x.float()).type_as(x)\n","        return out * self.weight # (2, 8, DIM) Values stays the same. We make the tensor grad_fn.\n","    \n","dummy_inp = torch.randn(2, 8, DIM)\n","norm = RMSNorm(DIM, NORM_EPS)\n","output = norm(dummy_inp)\n","print(output.shape)\n","\n","def precompute_freqs_cis(dim, end, theta = 10000.0):\n","    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n","    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n","    freqs = torch.outer(t, freqs)\n","    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n","    return freqs_cis\n","\n","\n","def reshape_for_broadcast(freqs_cis, x):\n","    ndim = x.ndim\n","    assert 0 <= 1 < ndim\n","    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n","    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n","    return freqs_cis.view(*shape)\n","\n","\n","def apply_rotary_emb(xq, xk, freqs_cis):\n","    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n","    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n","    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n","    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n","    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n","    return xq_out.type_as(xq), xk_out.type_as(xk)\n","\n","dummy_inp1 = torch.randn(2, 8, N_HEADS, HEAD_DIM)\n","dummy_inp2 = torch.randn(2, 8, N_KV_HEADS, HEAD_DIM)\n","\n","dummy_freqs_cis = precompute_freqs_cis(HEAD_DIM, MAX_SEQ_LEN*2, ROPE_THETA)\n","dummy_freqs_cis = dummy_freqs_cis[0 : 0 + 8]\n","\n","out1, out2 = apply_rotary_emb(dummy_inp1, dummy_inp2, dummy_freqs_cis)\n","print(\"-\"*30)\n","print(out1.shape)\n","print(out2.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:25.795979Z","iopub.status.busy":"2024-09-14T11:11:25.795604Z","iopub.status.idle":"2024-09-14T11:11:27.812286Z","shell.execute_reply":"2024-09-14T11:11:27.811427Z","shell.execute_reply.started":"2024-09-14T11:11:25.795938Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 8, 4096])\n"]}],"source":["class FeedForward(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Bias is false. It usually adds overhead to the transformer models.\n","        self.w1 = nn.Linear(DIM, FFN_DIM, bias=False)\n","        self.w3 = nn.Linear(DIM, FFN_DIM, bias=False)\n","        self.w2 = nn.Linear(FFN_DIM, DIM, bias=False)\n","\n","    def forward(self, x):\n","        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (2, 8, DIM) = (bsz, seqlen, DIM) - use the SwiGLU activation function (llama3) Table 3.\n","    \n","dummy_inp = torch.randn(2, 8, DIM)\n","\n","feed_forward = FeedForward()\n","\n","output = feed_forward(dummy_inp)\n","del feed_forward\n","print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:27.814723Z","iopub.status.busy":"2024-09-14T11:11:27.813978Z","iopub.status.idle":"2024-09-14T11:11:28.727891Z","shell.execute_reply":"2024-09-14T11:11:28.727010Z","shell.execute_reply.started":"2024-09-14T11:11:27.814678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 8, 4096])\n"]}],"source":["# GQA With Cache\n","class Attention(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.wq = nn.Linear(DIM, N_HEADS * HEAD_DIM, bias=False)\n","        self.wk = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n","        self.wv = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n","        self.wo = nn.Linear(N_HEADS * HEAD_DIM, DIM, bias=False) # Weight matrix defined in the MultiheadAttention formula.\n","\n","        # Create empty caches for keys and values.\n","        self.cache_k = torch.zeros(\n","            (\n","                MAX_BATCH_SIZE,\n","                MAX_SEQ_LEN,\n","                N_KV_HEADS,\n","                HEAD_DIM,\n","            )\n","        )\n","        self.cache_v = torch.zeros(\n","            (\n","                MAX_BATCH_SIZE,\n","                MAX_SEQ_LEN,\n","                N_KV_HEADS,\n","                HEAD_DIM,\n","            )\n","        )\n","\n","    def forward(self, x, start_pos, freqs_cis, mask):\n","        bsz, seqlen, _ = x.shape # Get batch size and sequence length. (bsz, seqlen, DIM)\n","        queries, keys, values = self.wq(x), self.wk(x), self.wv(x) # q -> (bsz, seqlen, N_HEADS*HEAD_DIM) | k,v -> (bsz, seqlen, N_KV_HEADS*HEAD_DIM)\n","\n","        queries = queries.view(bsz, seqlen, N_HEADS, HEAD_DIM)\n","        keys = keys.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n","        values = values.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n","\n","        queries, keys = apply_rotary_emb(queries, keys, freqs_cis=freqs_cis)\n","\n","        self.cache_k = self.cache_k.to(queries.device)\n","        self.cache_v = self.cache_v.to(queries.device)\n","\n","        self.cache_k[:bsz, start_pos : start_pos + seqlen] = keys\n","        self.cache_v[:bsz, start_pos : start_pos + seqlen] = values\n","\n","        keys = self.cache_k[:bsz, : start_pos + seqlen]\n","        values = self.cache_v[:bsz, : start_pos + seqlen]\n","\n","        # In these runs we simply duplicated the KV heads for MQA in all GPUs. (llama2)\n","        keys = torch.repeat_interleave(\n","            keys, dim=2, repeats=N_KV_HEAD_REP\n","        ) # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n","        values = torch.repeat_interleave(\n","            values, dim=2, repeats=N_KV_HEAD_REP\n","        )  # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n","\n","        # Reshaping for scaled_dot_product_attention. (bsz, ..., seqlen, HEAD_DIM) expected.\n","        queries = queries.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n","        keys = keys.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n","        values = values.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n","\n","        out = F.scaled_dot_product_attention(\n","            queries,\n","            keys,\n","            values,\n","            attn_mask=mask,\n","        ) # (bsz, N_HEADS, seqlen, HEAD_DIM)\n","        \n","        \n","        # If we don't use `contiguous` torch may complain.\n","        out = out.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # transpose, (bsz, seqlen, N_HEADS, HEAD_DIM) - (bsz, seqlen, DIM) - -1 does N_HEAD * HEAD_DIM = DIM\n","        return self.wo(out) # (bsz, seqlen, DIM)\n","    \n","dummy_inp = torch.randn(2, 8, DIM) # 8 is sequence length. Depends on your input. I put 8.\n","dummy_start_pos = 0\n","dummy_freqs_cis = torch.randn(8, 64)\n","dummy_mask = torch.randn(8, 8) # Mask is size (seqlen, seqlen)\n","\n","attention = Attention()\n","\n","output = attention(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\n","print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)\n","del attention"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:28.730185Z","iopub.status.busy":"2024-09-14T11:11:28.729478Z","iopub.status.idle":"2024-09-14T11:11:31.398547Z","shell.execute_reply":"2024-09-14T11:11:31.397556Z","shell.execute_reply.started":"2024-09-14T11:11:28.730142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 8, 4096])\n"]}],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.attention = Attention()\n","        self.feed_forward = FeedForward()\n","        self.attention_norm = RMSNorm(DIM, NORM_EPS)\n","        self.ffn_norm = RMSNorm(DIM, NORM_EPS)\n","\n","    def forward(self, x, start_pos, freqs_cis, mask):\n","        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n","        out = h + self.feed_forward(self.ffn_norm(h)) # (2, 8, DIM) = (bsz, seqlen, DIM)\n","        return out # (2, 8, DIM) = (bsz, seqlen, DIM)\n","    \n","dummy_inp = torch.randn(2, 8, DIM)\n","dummy_start_pos = 0\n","dummy_freqs_cis = torch.randn(8, 64)\n","dummy_mask = torch.randn(8, 8)\n","\n","transformer_block = TransformerBlock()\n","\n","output = transformer_block(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\n","print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:11:31.400608Z","iopub.status.busy":"2024-09-14T11:11:31.399968Z","iopub.status.idle":"2024-09-14T11:13:05.999896Z","shell.execute_reply":"2024-09-14T11:13:05.998799Z","shell.execute_reply.started":"2024-09-14T11:11:31.400553Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 8, 128256])\n"]}],"source":["class Transformer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.tok_embeddings = nn.Embedding(\n","            VOCAB_SIZE, DIM\n","        )\n","        \n","        self.layers = torch.nn.ModuleList()\n","        for _ in range(N_LAYERS):\n","            self.layers.append(TransformerBlock())\n","\n","        self.norm = RMSNorm(DIM, NORM_EPS)\n","        self.output = nn.Linear(DIM, VOCAB_SIZE, bias=False,)\n","\n","        self.freqs_cis = precompute_freqs_cis(\n","            HEAD_DIM,\n","            MAX_SEQ_LEN * 2,\n","            ROPE_THETA,\n","        )\n","\n","    @torch.inference_mode()\n","    def forward(self, tokens, start_pos):       \n","        _bsz, seqlen = tokens.shape\n","        h = self.tok_embeddings(tokens) # (bsz, seqlen, DIM)\n","        self.freqs_cis = self.freqs_cis.to(tokens.device)\n","        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n","\n","        mask = None # When we take the tokens from the cached values (seqlen=1) we don't need any aditional mask.\n","        if seqlen > 1: # Because of KV Cache, we process only 1 token. However, the first run doesn't have any cache. So it has a seqlen > 1.\n","            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device) # Since this is the first pass, we don't have any KV Cache. So we need a mask. Create (seqlen, seqlen) matrix with float(\"-inf\") values.\n","\n","            mask = torch.triu(mask, diagonal=1).to(tokens.device) # Take the upper triangle excluding diagonal since it's casual LM.\n","\n","        for layer in self.layers:\n","            h = layer(h, start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n","        h = self.norm(h) # (2, 8, 4096) = (bsz, seqlen, DIM)\n","        out = self.output(h).float() # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n","        return out # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n","    \n","dummy_tokens = torch.rand(2, 8).long() # Use `rand` instead of `randn`. `randn` generates negative number which is invalid for `nn.Embedding`\n","dummy_start_pos = 0\n","\n","transformer = Transformer()\n","\n","output = transformer(dummy_tokens, dummy_start_pos)\n","print(output.shape) # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n","del transformer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:13:06.002979Z","iopub.status.busy":"2024-09-14T11:13:06.002641Z","iopub.status.idle":"2024-09-14T11:13:06.169956Z","shell.execute_reply":"2024-09-14T11:13:06.168958Z","shell.execute_reply.started":"2024-09-14T11:13:06.002944Z"},"trusted":true},"outputs":[],"source":["# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n","\n","import os\n","from logging import getLogger\n","from pathlib import Path\n","from typing import (\n","    AbstractSet,\n","    cast,\n","    Collection,\n","    Dict,\n","    Iterator,\n","    List,\n","    Literal,\n","    Sequence,\n","    TypedDict,\n","    Union,\n",")\n","\n","\n","logger = getLogger(__name__)\n","import tiktoken\n","from tiktoken.load import load_tiktoken_bpe\n","\n","Role = Literal[\"system\", \"user\", \"assistant\"]\n","\n","\n","class Message(TypedDict):\n","    role: Role\n","    content: str\n","\n","\n","Dialog = Sequence[Message]\n","\n","\n","class Tokenizer:\n","    \"\"\"\n","    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n","    \"\"\"\n","\n","    special_tokens: Dict[str, int]\n","\n","    num_reserved_special_tokens = 256\n","\n","    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n","\n","    def __init__(self, model_path: str):\n","        \"\"\"\n","        Initializes the Tokenizer with a Tiktoken model.\n","\n","        Args:\n","            model_path (str): The path to the Tiktoken model file.\n","        \"\"\"\n","        assert os.path.isfile(model_path), model_path\n","\n","        mergeable_ranks = load_tiktoken_bpe(model_path)\n","        num_base_tokens = len(mergeable_ranks)\n","        special_tokens = [\n","            \"<|begin_of_text|>\",\n","            \"<|end_of_text|>\",\n","            \"<|reserved_special_token_0|>\",\n","            \"<|reserved_special_token_1|>\",\n","            \"<|reserved_special_token_2|>\",\n","            \"<|reserved_special_token_3|>\",\n","            \"<|start_header_id|>\",\n","            \"<|end_header_id|>\",\n","            \"<|reserved_special_token_4|>\",\n","            \"<|eot_id|>\",  # end of turn\n","        ] + [\n","            f\"<|reserved_special_token_{i}|>\"\n","            for i in range(5, self.num_reserved_special_tokens - 5)\n","        ]\n","        self.special_tokens = {\n","            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n","        }\n","        self.model = tiktoken.Encoding(\n","            name=Path(model_path).name,\n","            pat_str=self.pat_str,\n","            mergeable_ranks=mergeable_ranks,\n","            special_tokens=self.special_tokens,\n","        )\n","        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n","\n","        self.n_words: int = self.model.n_vocab\n","        # BOS / EOS token IDs\n","        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n","        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n","        self.pad_id: int = -1\n","        self.stop_tokens = {\n","            self.special_tokens[\"<|end_of_text|>\"],\n","            self.special_tokens[\"<|eot_id|>\"],\n","        }\n","        logger.info(\n","            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n","        )\n","\n","    def encode(\n","        self,\n","        s: str,\n","        *,\n","        bos: bool,\n","        eos: bool,\n","        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n","        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n","    ) -> List[int]:\n","        \"\"\"\n","        Encodes a string into a list of token IDs.\n","\n","        Args:\n","            s (str): The input string to be encoded.\n","            bos (bool): Whether to prepend the beginning-of-sequence token.\n","            eos (bool): Whether to append the end-of-sequence token.\n","            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n","            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n","\n","        Returns:\n","            list[int]: A list of token IDs.\n","\n","        By default, setting disallowed_special=() encodes a string by ignoring\n","        special tokens. Specifically:\n","        - Setting `disallowed_special` to () will cause all text corresponding\n","          to special tokens to be encoded as natural text (insteading of raising\n","          an error).\n","        - Setting `allowed_special` to \"all\" will treat all text corresponding\n","          to special tokens to be encoded as special tokens.\n","        \"\"\"\n","        assert type(s) is str\n","\n","        # The tiktoken tokenizer can handle <=400k chars without\n","        # pyo3_runtime.PanicException.\n","        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n","\n","        # https://github.com/openai/tiktoken/issues/195\n","        # Here we iterate over subsequences and split if we exceed the limit\n","        # of max consecutive non-whitespace or whitespace characters.\n","        MAX_NO_WHITESPACES_CHARS = 25_000\n","\n","        substrs = (\n","            substr\n","            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n","            for substr in self._split_whitespaces_or_nonwhitespaces(\n","                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n","            )\n","        )\n","        t: List[int] = []\n","        for substr in substrs:\n","            t.extend(\n","                self.model.encode(\n","                    substr,\n","                    allowed_special=allowed_special,\n","                    disallowed_special=disallowed_special,\n","                )\n","            )\n","        if bos:\n","            t.insert(0, self.bos_id)\n","        if eos:\n","            t.append(self.eos_id)\n","        return t\n","\n","    def decode(self, t: Sequence[int]) -> str:\n","        \"\"\"\n","        Decodes a list of token IDs into a string.\n","\n","        Args:\n","            t (List[int]): The list of token IDs to be decoded.\n","\n","        Returns:\n","            str: The decoded string.\n","        \"\"\"\n","        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n","        return self.model.decode(cast(List[int], t))\n","\n","    @staticmethod\n","    def _split_whitespaces_or_nonwhitespaces(\n","        s: str, max_consecutive_slice_len: int\n","    ) -> Iterator[str]:\n","        \"\"\"\n","        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n","        consecutive whitespaces or consecutive non-whitespaces.\n","        \"\"\"\n","        current_slice_len = 0\n","        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n","        slice_start = 0\n","\n","        for i in range(len(s)):\n","            is_now_space = s[i].isspace()\n","\n","            if current_slice_is_space ^ is_now_space:\n","                current_slice_len = 1\n","                current_slice_is_space = is_now_space\n","            else:\n","                current_slice_len += 1\n","                if current_slice_len > max_consecutive_slice_len:\n","                    yield s[slice_start:i]\n","                    slice_start = i\n","                    current_slice_len = 1\n","        yield s[slice_start:]\n","\n","\n","class ChatFormat:\n","    def __init__(self, tokenizer: Tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def encode_header(self, message: Message) -> List[int]:\n","        tokens = []\n","        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n","        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n","        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n","        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n","        return tokens\n","\n","    def encode_message(self, message: Message) -> List[int]:\n","        tokens = self.encode_header(message)\n","        tokens.extend(\n","            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n","        )\n","        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n","        return tokens\n","\n","    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n","        tokens = []\n","        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n","        for message in dialog:\n","            tokens.extend(self.encode_message(message))\n","        # Add the start of an assistant message for the model to complete.\n","        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n","        return tokens\n","\n","# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n","\n","import json\n","import os\n","import sys\n","import time\n","from pathlib import Path\n","from typing import List, Optional, Tuple, TypedDict\n","\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class CompletionPrediction(TypedDict, total=False):\n","    generation: str\n","    tokens: List[str]  # not required\n","    logprobs: List[float]  # not required\n","\n","\n","class ChatPrediction(TypedDict, total=False):\n","    generation: Message\n","    tokens: List[str]  # not required\n","    logprobs: List[float]  # not required\n","\n","\n","class Llama:\n","    @staticmethod\n","    def build(\n","        ckpt_dir: str,\n","        tokenizer_path: str,\n","        max_seq_len: int,\n","        max_batch_size: int,\n","        model_parallel_size: Optional[int] = None,\n","        seed: int = 1,\n","    ) -> \"Llama\":\n","        \"\"\"\n","        Build a Llama instance by initializing and loading a model checkpoint.\n","\n","        Args:\n","            ckpt_dir (str): Path to the directory containing checkpoint files.\n","            tokenizer_path (str): Path to the tokenizer file.\n","            max_seq_len (int): Maximum sequence length for input text.\n","            max_batch_size (int): Maximum batch size for inference.\n","            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n","                If not provided, it's determined from the environment. Defaults to None.\n","\n","        Returns:\n","            Llama: An instance of the Llama class with the loaded model and tokenizer.\n","\n","        Raises:\n","            AssertionError: If there are no checkpoint files in the specified directory,\n","                or if the model parallel size does not match the number of checkpoint files.\n","\n","        Note:\n","            This method initializes the distributed process group, sets the device to CUDA,\n","            and loads the pre-trained model and tokenizer.\n","        \"\"\"\n","        assert 1 <= max_seq_len <= 8192, f\"max_seq_len must be between 1 and 8192, got {max_seq_len}.\"\n","        assert os.path.isdir(ckpt_dir), f\"Checkpoint directory '{ckpt_dir}' does not exist.\"\n","        assert os.path.isfile(tokenizer_path), f\"Tokenizer file '{tokenizer_path}' does not exist.\"\n","\n","        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n","        torch.cuda.set_device(local_rank)\n","\n","        # seed must be the same in all processes\n","        torch.manual_seed(seed)\n","\n","        if local_rank > 0:\n","            sys.stdout = open(os.devnull, \"w\")\n","\n","        start_time = time.time()\n","        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n","        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n","        checkpoint = torch.load(ckpt_dir+\"consolidated.00.pth\", map_location=\"cpu\")\n","        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n","            params = json.loads(f.read())\n","            \n","        tokenizer = Tokenizer(model_path=tokenizer_path)\n","        assert VOCAB_SIZE == tokenizer.n_words\n","        if torch.cuda.is_bf16_supported():\n","            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n","        else:\n","            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n","        model = Transformer()\n","        print(f\"PARAMETERS: {sum(p.numel() for p in model.parameters())}\")\n","        model.load_state_dict(checkpoint, strict=False)\n","        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n","\n","        return Llama(model, tokenizer)\n","\n","    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.formatter = ChatFormat(tokenizer)\n","\n","    @torch.inference_mode()\n","    def generate(\n","        self,\n","        prompt_tokens: List[List[int]],\n","        max_gen_len: int,\n","        temperature: float = 0.6,\n","        top_p: float = 0.9,\n","        logprobs: bool = False,\n","        echo: bool = False,\n","    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n","        \"\"\"\n","        Generate text sequences based on provided prompts using the language generation model.\n","\n","        Args:\n","            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n","            max_gen_len (int): Maximum length of the generated text sequence.\n","            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n","            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n","            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n","            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n","\n","        Returns:\n","            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n","\n","        Note:\n","            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n","            If logprobs is True, token log probabilities are computed for each generated token.\n","\n","        \"\"\"\n","        bsz = len(prompt_tokens)\n","        assert bsz <= MAX_BATCH_SIZE, (bsz, MAX_BATCH_SIZE)\n","\n","        min_prompt_len = min(len(t) for t in prompt_tokens)\n","        max_prompt_len = max(len(t) for t in prompt_tokens)\n","        assert max_prompt_len <= MAX_SEQ_LEN\n","        total_len = min(MAX_SEQ_LEN, max_gen_len + max_prompt_len)\n","\n","        pad_id = self.tokenizer.pad_id\n","        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n","        for k, t in enumerate(prompt_tokens):\n","            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n","        if logprobs:\n","            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n","\n","        prev_pos = 0\n","        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n","        input_text_mask = tokens != pad_id\n","        if min_prompt_len == total_len:\n","            logits = self.model.forward(tokens, prev_pos)\n","            token_logprobs = -F.cross_entropy(\n","                input=logits.transpose(1, 2),\n","                target=tokens,\n","                reduction=\"none\",\n","                ignore_index=pad_id,\n","            )\n","\n","        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n","\n","        for cur_pos in range(min_prompt_len, total_len):\n","            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n","            if temperature > 0:\n","                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n","                next_token = sample_top_p(probs, top_p)\n","            else:\n","                next_token = torch.argmax(logits[:, -1], dim=-1)\n","\n","            next_token = next_token.reshape(-1)\n","            # only replace token if prompt has already been generated\n","            next_token = torch.where(\n","                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n","            )\n","            tokens[:, cur_pos] = next_token\n","            if logprobs:\n","                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n","                    input=logits.transpose(1, 2),\n","                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n","                    reduction=\"none\",\n","                    ignore_index=pad_id,\n","                )\n","            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n","                torch.isin(next_token, stop_tokens)\n","            )\n","            prev_pos = cur_pos\n","            if all(eos_reached):\n","                break\n","\n","        if logprobs:\n","            token_logprobs = token_logprobs.tolist()\n","        out_tokens, out_logprobs = [], []\n","        for i, toks in enumerate(tokens.tolist()):\n","            # cut to max gen len\n","            start = 0 if echo else len(prompt_tokens[i])\n","            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n","            probs = None\n","            if logprobs:\n","                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n","            # cut to after eos tok if any\n","            for stop_token in self.tokenizer.stop_tokens:\n","                try:\n","                    eos_idx = toks.index(stop_token)\n","                    toks = toks[:eos_idx]\n","                    probs = probs[:eos_idx] if logprobs else None\n","                except ValueError:\n","                    pass\n","            out_tokens.append(toks)\n","            out_logprobs.append(probs)\n","        return (out_tokens, out_logprobs if logprobs else None)\n","\n","    def text_completion(\n","        self,\n","        prompts: List[str],\n","        temperature: float = 0.6,\n","        top_p: float = 0.9,\n","        max_gen_len: Optional[int] = None,\n","        logprobs: bool = False,\n","        echo: bool = False,\n","    ) -> List[CompletionPrediction]:\n","        \"\"\"\n","        Perform text completion for a list of prompts using the language generation model.\n","\n","        Args:\n","            prompts (List[str]): List of text prompts for completion.\n","            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n","            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n","            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n","                If not provided, it's set to the model's maximum sequence length minus 1.\n","            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n","            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n","\n","        Returns:\n","            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n","\n","        Note:\n","            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n","            If logprobs is True, token log probabilities are computed for each generated token.\n","\n","        \"\"\"\n","        if max_gen_len is None:\n","            max_gen_len = MAX_SEQ_LEN - 1\n","        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n","        generation_tokens, generation_logprobs = self.generate(\n","            prompt_tokens=prompt_tokens,\n","            max_gen_len=max_gen_len,\n","            temperature=temperature,\n","            top_p=top_p,\n","            logprobs=logprobs,\n","            echo=echo,\n","        )\n","        if logprobs:\n","            return [\n","                {\n","                    \"generation\": self.tokenizer.decode(t),\n","                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n","                    \"logprobs\": logprobs_i,\n","                }\n","                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n","            ]\n","        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n","\n","    def chat_completion(\n","        self,\n","        dialogs: List[Dialog],\n","        temperature: float = 0.6,\n","        top_p: float = 0.9,\n","        max_gen_len: Optional[int] = None,\n","        logprobs: bool = False,\n","    ) -> List[ChatPrediction]:\n","        \"\"\"\n","        Generate assistant responses for a list of conversational dialogs using the language generation model.\n","\n","        Args:\n","            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n","            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n","            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n","            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n","                If not provided, it's set to the model's maximum sequence length minus 1.\n","            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n","\n","        Returns:\n","            List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n","\n","        Note:\n","            This method generates assistant responses for the provided conversational dialogs.\n","            It employs nucleus sampling to introduce controlled randomness in text generation.\n","            If logprobs is True, token log probabilities are computed for each generated token.\n","        \"\"\"\n","        if max_gen_len is None:\n","            max_gen_len = MAX_SEQ_LEN - 1\n","\n","        prompt_tokens = [\n","            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n","        ]\n","        generation_tokens, generation_logprobs = self.generate(\n","            prompt_tokens=prompt_tokens,\n","            max_gen_len=max_gen_len,\n","            temperature=temperature,\n","            top_p=top_p,\n","            logprobs=logprobs,\n","        )\n","        if logprobs:\n","            return [\n","                {\n","                    \"generation\": {\n","                        \"role\": \"assistant\",\n","                        \"content\": self.tokenizer.decode(t),\n","                    },\n","                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n","                    \"logprobs\": logprobs_i,\n","                }\n","                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n","            ]\n","        return [\n","            {\n","                \"generation\": {\n","                    \"role\": \"assistant\",\n","                    \"content\": self.tokenizer.decode(t),\n","                },\n","            }\n","            for t in generation_tokens\n","        ]\n","\n","\n","def sample_top_p(probs, p):\n","    \"\"\"\n","    Perform top-p (nucleus) sampling on a probability distribution.\n","\n","    Args:\n","        probs (torch.Tensor): Probability distribution tensor.\n","        p (float): Probability threshold for top-p sampling.\n","\n","    Returns:\n","        torch.Tensor: Sampled token indices.\n","\n","    Note:\n","        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n","        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n","    \"\"\"\n","    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n","    probs_sum = torch.cumsum(probs_sort, dim=-1)\n","    mask = probs_sum - probs_sort > p\n","    probs_sort[mask] = 0.0\n","    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n","    next_token = torch.multinomial(probs_sort, num_samples=1)\n","    next_token = torch.gather(probs_idx, -1, next_token)\n","    return next_token"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T11:15:54.329480Z","iopub.status.busy":"2024-09-14T11:15:54.328545Z","iopub.status.idle":"2024-09-14T11:18:45.597323Z","shell.execute_reply":"2024-09-14T11:18:45.596318Z","shell.execute_reply.started":"2024-09-14T11:15:54.329441Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      8\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 10\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# For these prompts, the expected answer is the natural continuation of the prompt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI believe the meaning of life is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimply put, the theory of relativity states that \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     23\u001b[0m results \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mtext_completion(\n\u001b[1;32m     24\u001b[0m     prompts,\n\u001b[1;32m     25\u001b[0m     max_gen_len\u001b[38;5;241m=\u001b[39mmax_gen_len,\n\u001b[1;32m     26\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     27\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m     28\u001b[0m )\n","Cell \u001b[0;32mIn[10], line 313\u001b[0m, in \u001b[0;36mLlama.build\u001b[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_tensor_type(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mHalfTensor)\n\u001b[0;32m--> 313\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARAMETERS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    315\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIM\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_LAYERS):\n","File \u001b[0;32m~/.conda/envs/irm/lib/python3.10/site-packages/torch/nn/modules/sparse.py:143\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    144\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["ckpt_dir = \"/home/huang717/.llama/checkpoints/Llama-3-8B/\"\n","tokenizer_path = \"/home/huang717/.llama/checkpoints/Llama-3-8B/tokenizer.model\"\n","        \n","temperature = 0.6\n","top_p = 0.9\n","max_seq_len = 128\n","max_gen_len = 64\n","max_batch_size = 4\n","\n","generator = Llama.build(\n","    ckpt_dir=ckpt_dir,\n","    tokenizer_path=tokenizer_path,\n","    max_seq_len=max_seq_len,\n","    max_batch_size=max_batch_size,\n","    model_parallel_size=1\n",")\n","\n","prompts = [\n","    # For these prompts, the expected answer is the natural continuation of the prompt\n","    \"I believe the meaning of life is\",\n","    \"Simply put, the theory of relativity states that \",\n","]\n","results = generator.text_completion(\n","    prompts,\n","    max_gen_len=max_gen_len,\n","    temperature=temperature,\n","    top_p=top_p,\n",")\n","for prompt, result in zip(prompts, results):\n","    print(prompt)\n","    print(f\"> {result['generation']}\")\n","    print(\"\\n==================================\\n\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["ckpt_dir = \"/home/huang717/.llama/checkpoints/Llama-3-8B/\"\n","assert os.path.isdir(ckpt_dir), f\"Checkpoint directory '{ckpt_dir}' does not exist.\""]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5417429,"sourceId":8994037,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"irm","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
